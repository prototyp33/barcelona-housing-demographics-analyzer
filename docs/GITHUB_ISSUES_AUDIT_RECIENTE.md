# GitHub Issues - Audit de Archivos Recientes

**Fecha:** 2025-12-01  
**Alcance:** Archivos recientes del proyecto (transformaciones ETL, pipeline, extracci√≥n, workflows)

---

## Issues Nuevos Identificados

### Issue 1: Bug en regex de `_parse_household_size` (utils.py)

**Severidad:** üü° Media  
**Etiquetas:** `bug`, `etl`, `data-processing`, `priority-medium`

**Descripci√≥n:**

La funci√≥n `_parse_household_size` en `src/etl/transformations/utils.py` usa un regex con doble backslash (`r"\\d+"`) cuando deber√≠a usar un solo backslash (`r"\d+"`). En Python raw strings, el doble backslash puede causar que el regex no funcione correctamente.

**Archivo afectado:**
- `src/etl/transformations/utils.py:46, 52, 58`

**C√≥digo problem√°tico:**
```python
if normalized.startswith(">"):
    digits = re.findall(r"\\d+", normalized)  # ‚ùå Doble backslash
```

**Impacto:**
- Puede fallar al parsear tama√±os de hogar que contengan n√∫meros
- Afecta el c√°lculo de m√©tricas de hogares en `enrich_fact_demografia`

**Soluci√≥n propuesta:**
```python
if normalized.startswith(">"):
    digits = re.findall(r"\d+", normalized)  # ‚úÖ Backslash simple
```

**Pasos para resolver:**
- [ ] Corregir regex en l√≠nea 46
- [ ] Corregir regex en l√≠nea 52  
- [ ] Corregir regex en l√≠nea 58
- [ ] A√±adir test unitario que verifique parsing de tama√±os de hogar con n√∫meros

---

### Issue 2: Manejo de errores gen√©rico en `enrichment.py`

**Severidad:** üü° Media  
**Etiquetas:** `code-quality`, `etl`, `priority-medium`

**Descripci√≥n:**

M√∫ltiples bloques `except Exception` en `src/etl/transformations/enrichment.py` capturan excepciones muy amplias sin especificar tipos concretos, lo que dificulta el debugging y puede ocultar errores inesperados.

**Archivos afectados:**
- `src/etl/transformations/enrichment.py:52` (carga de metadatos)
- `src/etl/transformations/enrichment.py:164` (procesamiento de CSV)

**C√≥digo problem√°tico:**
```python
except Exception as exc:  # noqa: BLE001
    logger.warning("Error cargando metadatos: %s", exc)
```

**Impacto:**
- Errores cr√≠ticos pueden ser tratados como warnings
- Dificulta identificar la causa ra√≠z de fallos
- No sigue las mejores pr√°cticas del proyecto (ver CODE_AUDIT_ISSUES.md #6)

**Soluci√≥n propuesta:**
```python
except (FileNotFoundError, pd.errors.ParserError, json.JSONDecodeError) as exc:
    logger.warning("Error cargando metadatos: %s", exc, exc_info=True)
except Exception as exc:
    logger.error("Error inesperado cargando metadatos: %s", exc, exc_info=True)
    raise
```

**Pasos para resolver:**
- [ ] Identificar tipos de excepciones espec√≠ficos para cada bloque try/except
- [ ] Reemplazar `except Exception` por tipos concretos donde sea posible
- [ ] A√±adir `exc_info=True` a logs de errores
- [ ] Evaluar si algunos errores deber√≠an detener el pipeline en lugar de continuar

---

### Issue 3: Import no utilizado en `enrichment.py`

**Severidad:** üü¢ Baja  
**Etiquetas:** `code-quality`, `cleanup`, `priority-low`

**Descripci√≥n:**

El m√≥dulo `src/etl/transformations/enrichment.py` importa `json` pero est√° marcado como no utilizado (`# noqa: F401`) con el comentario "se mantiene por compatibilidad si se usa en futuras extensiones". Esto es c√≥digo muerto que deber√≠a eliminarse o documentarse mejor.

**Archivo afectado:**
- `src/etl/transformations/enrichment.py:38`

**C√≥digo problem√°tico:**
```python
import json  # noqa: F401  # se mantiene por compatibilidad si se usa en futuras extensiones
```

**Impacto:**
- C√≥digo muerto que confunde a linters y desarrolladores
- Import innecesario aumenta tiempo de carga del m√≥dulo

**Soluci√≥n propuesta:**
- Opci√≥n 1: Eliminar el import si realmente no se usa
- Opci√≥n 2: Si se planea usar en el futuro, moverlo a donde se necesite cuando se implemente

**Pasos para resolver:**
- [ ] Verificar que `json` no se usa en ninguna parte del m√≥dulo
- [ ] Eliminar el import si no se necesita
- [ ] Si se necesita en el futuro, a√±adirlo cuando se implemente la funcionalidad

---

### Issue 4: Validaci√≥n faltante en `prepare_fact_precios` para pipes duplicados

**Severidad:** üü° Media  
**Etiquetas:** `bug`, `etl`, `data-quality`, `priority-medium`

**‚ö†Ô∏è Relacionada con:** Issue #13 "Fix: Deduplicaci√≥n agresiva en fact_precios"

**Descripci√≥n:**

La funci√≥n `prepare_fact_precios` detecta pipes duplicados (`|`) en las columnas `source` y `dataset_id` pero solo loguea un error sin corregir el problema. Aunque existe la funci√≥n `_normalize_pipe_tags` que puede normalizar estos valores, no se aplica autom√°ticamente cuando se detecta el problema.

**Archivo afectado:**
- `src/etl/transformations/market.py:243-252`

**C√≥digo problem√°tico:**
```python
if fact["source"].astype(str).str.contains(r"\\|").any():
    logger.error(
        "‚ö†Ô∏è ALERTA: Se detectaron pipes '|' en columna 'source'. "
        "Esto indica un problema de agregaci√≥n upstream.",
    )
# Solo loguea error, no corrige
```

**Impacto:**
- Datos con pipes duplicados pueden persistir en la base de datos
- Puede causar problemas en consultas y an√°lisis posteriores
- La funci√≥n `_normalize_pipe_tags` ya existe pero no se aplica aqu√≠

**Soluci√≥n propuesta:**
```python
# Aplicar normalizaci√≥n autom√°ticamente despu√©s de detectar el problema
if fact["source"].astype(str).str.contains(r"\\|").any():
    logger.warning(
        "Se detectaron pipes duplicados en 'source'. Normalizando autom√°ticamente."
    )
    fact["source"] = fact["source"].apply(_normalize_pipe_tags)
```

**Pasos para resolver:**
- [ ] Aplicar `_normalize_pipe_tags` autom√°ticamente cuando se detecten pipes duplicados
- [ ] Cambiar log level de ERROR a WARNING si se corrige autom√°ticamente
- [ ] A√±adir test que verifique la correcci√≥n autom√°tica de pipes duplicados
- [ ] Documentar el comportamiento en docstring de la funci√≥n

---

### Issue 5: Tests marcados como skip en `test_pipeline.py`

**Severidad:** üü° Media  
**Etiquetas:** `testing`, `etl`, `priority-medium`

**‚ö†Ô∏è Relacionada con:** Issue #20 "Task: Testing - Unit e Integration Tests", Issue #40 "Tests de integraci√≥n para pipeline ETL"

**Descripci√≥n:**

M√∫ltiples tests en `tests/test_pipeline.py` est√°n marcados con `@pytest.mark.skip` porque requieren datos con estructura exacta del esquema real. Esto reduce la cobertura de tests y puede ocultar regresiones.

**Archivo afectado:**
- `tests/test_pipeline.py:117, 141, 179, 209, 249`

**Tests afectados:**
- `test_etl_creates_database` (l√≠nea 117)
- `test_etl_creates_dim_barrios` (l√≠nea 141)
- `test_etl_creates_fact_precios` (l√≠nea 179)
- `test_etl_creates_fact_demografia` (l√≠nea 209)
- `test_etl_registers_run` (l√≠nea 249)

**Problema:**
```python
@pytest.mark.skip(reason="Requiere datos con estructura exacta del esquema real.")
def test_etl_creates_database(raw_data_structure: Dict[str, Path]) -> None:
```

**Impacto:**
- Cobertura de tests reducida para el pipeline ETL cr√≠tico
- Regresiones pueden pasar desapercibidas
- Fixtures de prueba no son suficientemente robustos

**Soluci√≥n propuesta:**
- Crear fixtures m√°s robustos que generen datos con estructura v√°lida
- O documentar claramente c√≥mo generar datos de prueba v√°lidos
- O crear tests de integraci√≥n separados que usen datos reales (m√°s lentos pero m√°s completos)

**Pasos para resolver:**
- [ ] Revisar fixtures existentes en `test_pipeline.py`
- [ ] Crear fixtures que generen datos con estructura exacta del esquema
- [ ] Actualizar tests para usar fixtures mejorados
- [ ] Remover `@pytest.mark.skip` cuando los tests pasen
- [ ] Documentar c√≥mo generar datos de prueba v√°lidos si es necesario

---

### Issue 6: Manejo de errores silencioso en `pipeline.py`

**Severidad:** üü° Media  
**Etiquetas:** `code-quality`, `etl`, `error-handling`, `priority-medium`

**‚ö†Ô∏è Relacionada con:** Issue #43 "Refactor: Limpiar orquestador Pipeline"

**Descripci√≥n:**

M√∫ltiples bloques `try/except` en `src/etl/pipeline.py` solo loguean warnings pero contin√∫an la ejecuci√≥n del pipeline, incluso cuando algunos errores podr√≠an ser cr√≠ticos y deber√≠an detener el proceso.

**Archivo afectado:**
- `src/etl/pipeline.py` (m√∫ltiples ubicaciones)

**Ubicaciones problem√°ticas:**
- L√≠nea 252: Error cargando datos de renta
- L√≠nea 310: Error procesando demograf√≠a ampliada
- L√≠nea 363: Error procesando Portal de Dades
- L√≠nea 394: Error procesando renta
- L√≠nea 410: Error cargando Idealista venta
- L√≠nea 421: Error cargando Idealista alquiler
- L√≠nea 438: Error procesando oferta Idealista

**C√≥digo problem√°tico:**
```python
try:
    renta_df = _safe_read_csv(renta_path)
    logger.info("‚úì Datos de renta cargados: %s", renta_path.name)
except Exception as e:
    logger.warning("Error cargando datos de renta: %s", e)
    # Contin√∫a ejecuci√≥n sin datos de renta
```

**Impacto:**
- Errores cr√≠ticos pueden pasar desapercibidos
- Pipeline puede completarse "exitosamente" con datos incompletos
- Dificulta debugging de problemas de datos

**Soluci√≥n propuesta:**
- Clasificar errores en cr√≠ticos vs. opcionales
- Errores cr√≠ticos (ej: demograf√≠a base) deber√≠an detener el pipeline
- Errores opcionales (ej: Idealista, Portal de Dades) pueden continuar con warning
- A√±adir flag `--strict` para pipeline que falle en cualquier error

**Pasos para resolver:**
- [ ] Clasificar cada fuente de datos como cr√≠tica u opcional
- [ ] Modificar manejo de errores para fuentes cr√≠ticas (raise en lugar de warning)
- [ ] Mantener warnings para fuentes opcionales pero mejorar logging
- [ ] A√±adir `exc_info=True` a todos los logs de errores
- [ ] Documentar qu√© fuentes son cr√≠ticas vs. opcionales

---

### Issue 7: Falta validaci√≥n de a√±os en datos de Portal de Dades

**Severidad:** üü° Media  
**Etiquetas:** `bug`, `etl`, `data-quality`, `priority-medium`

**‚ö†Ô∏è Relacionada con:** Issue #15 "Improvement: Mejorar mapeo de territorios Portal de Dades"

**Descripci√≥n:**

La funci√≥n `_extract_year_from_temps` puede retornar `None` cuando falla el parsing de fechas, pero este valor no se valida antes de usarse en agrupaciones y operaciones que requieren a√±os v√°lidos.

**Archivo afectado:**
- `src/etl/transformations/enrichment.py:112`
- `src/etl/transformations/utils.py:114-120` (funci√≥n `_extract_year_from_temps`)

**C√≥digo problem√°tico:**
```python
df["anio"] = df["Dim-00:TEMPS"].apply(_extract_year_from_temps)
df = df.dropna(subset=["anio", "VALUE"])  # ‚úÖ Esto est√° bien
# Pero en otros lugares puede no validarse:
df.groupby(["anio", ...])  # ‚ùå Puede fallar si hay None
```

**Impacto:**
- Puede causar errores en agrupaciones por a√±o si hay valores None
- Datos con fechas inv√°lidas pueden ser procesados incorrectamente
- Puede causar errores silenciosos en c√°lculos temporales

**Soluci√≥n propuesta:**
- Validar que `anio` no sea None antes de agrupar
- A√±adir logging cuando se descarten registros por a√±o inv√°lido
- Documentar comportamiento esperado cuando `_extract_year_from_temps` retorna None

**Pasos para resolver:**
- [ ] Revisar todos los usos de `_extract_year_from_temps`
- [ ] Asegurar que siempre se valida `dropna(subset=["anio"])` antes de agrupar
- [ ] A√±adir logging cuando se descarten registros por a√±o inv√°lido
- [ ] A√±adir test que verifique manejo de a√±os inv√°lidos

---

### Issue 8: Workflow de dashboard-demo sin validaci√≥n de puerto

**Severidad:** üü¢ Baja  
**Etiquetas:** `ci-cd`, `workflow`, `priority-low`

**Descripci√≥n:**

El workflow `.github/workflows/dashboard-demo.yml` acepta un puerto como input string pero no valida que est√© en un rango v√°lido (1024-65535) antes de usarlo.

**Archivo afectado:**
- `.github/workflows/dashboard-demo.yml:10-14, 38`

**C√≥digo problem√°tico:**
```yaml
inputs:
  port:
    description: 'Streamlit port (default 8501)'
    required: false
    default: '8501'
    type: string  # ‚ùå No valida rango
```

**Impacto:**
- Puertos inv√°lidos pueden causar fallos en el workflow
- Puertos privilegiados (<1024) pueden causar errores de permisos
- Puertos fuera de rango pueden causar errores de conexi√≥n

**Soluci√≥n propuesta:**
```yaml
- name: Validate port
  run: |
    PORT="${{ inputs.port }}"
    if ! [[ "$PORT" =~ ^[0-9]+$ ]] || [ "$PORT" -lt 1024 ] || [ "$PORT" -gt 65535 ]; then
      echo "Error: Port must be between 1024 and 65535"
      exit 1
    fi
```

**Pasos para resolver:**
- [ ] A√±adir step de validaci√≥n de puerto antes de iniciar Streamlit
- [ ] Validar que el puerto sea num√©rico y est√© en rango v√°lido
- [ ] A√±adir mensaje de error claro si el puerto es inv√°lido
- [ ] Documentar rango v√°lido en la descripci√≥n del input

---

### Issue 9: Workflow kpi-update con manejo de errores gen√©rico

**Severidad:** üü¢ Baja  
**Etiquetas:** `ci-cd`, `workflow`, `code-quality`, `priority-low`

**Descripci√≥n:**

El workflow `.github/workflows/kpi-update.yml` usa `except:` sin especificar tipo de excepci√≥n, lo que es una mala pr√°ctica y puede ocultar errores inesperados.

**Archivo afectado:**
- `.github/workflows/kpi-update.yml:48`

**C√≥digo problem√°tico:**
```python
try:
    with open('$FILE', 'r') as f:
        data = json.load(f)
except:  # ‚ùå Bare except
    data = {"kpis": []}
```

**Impacto:**
- Puede capturar excepciones cr√≠ticas (KeyboardInterrupt, SystemExit)
- Dificulta debugging de errores reales
- No sigue mejores pr√°cticas de Python

**Soluci√≥n propuesta:**
```python
except (json.JSONDecodeError, FileNotFoundError) as e:
    logger.warning("Error cargando KPI progress, inicializando vac√≠o: %s", e)
    data = {"kpis": []}
```

**Pasos para resolver:**
- [ ] Especificar tipos de excepciones concretos (`json.JSONDecodeError`, `FileNotFoundError`)
- [ ] A√±adir logging del error para debugging
- [ ] Evaluar si otros errores deber√≠an propagarse

---

### Issue 10: Falta validaci√≥n de estructura de manifest.json

**Severidad:** üü° Media  
**Etiquetas:** `bug`, `etl`, `data-quality`, `priority-medium`

**Descripci√≥n:**

La funci√≥n `_load_manifest` en `src/etl/pipeline.py` carga el archivo JSON pero no valida que tenga la estructura esperada (lista de diccionarios con campos espec√≠ficos). Esto puede causar fallos silenciosos si el manifest tiene estructura incorrecta.

**Archivo afectado:**
- `src/etl/pipeline.py:45-67`

**C√≥digo problem√°tico:**
```python
def _load_manifest(raw_dir: Path) -> List[Dict[str, object]]:
    manifest_path = raw_dir / "manifest.json"
    if not manifest_path.exists():
        return []
    
    try:
        with open(manifest_path, 'r', encoding='utf-8') as f:
            manifest = json.load(f)  # ‚ùå No valida estructura
        return manifest
```

**Impacto:**
- Manifest con estructura incorrecta puede causar errores en tiempo de ejecuci√≥n
- Errores pueden ser dif√≠ciles de debuggear si el manifest est√° malformado
- Puede causar fallos silenciosos en descubrimiento de archivos

**Soluci√≥n propuesta:**
```python
def _load_manifest(raw_dir: Path) -> List[Dict[str, object]]:
    manifest_path = raw_dir / "manifest.json"
    if not manifest_path.exists():
        return []
    
    try:
        with open(manifest_path, 'r', encoding='utf-8') as f:
            manifest = json.load(f)
        
        # Validar estructura
        if not isinstance(manifest, list):
            logger.error("Manifest debe ser una lista, encontrado: %s", type(manifest))
            return []
        
        # Validar que cada entrada tenga campos m√≠nimos
        required_fields = {"file_path", "type"}
        for i, entry in enumerate(manifest):
            if not isinstance(entry, dict):
                logger.warning("Entrada %d del manifest no es un diccionario", i)
                continue
            missing = required_fields - set(entry.keys())
            if missing:
                logger.warning("Entrada %d del manifest falta campos: %s", i, missing)
        
        return manifest
    except json.JSONDecodeError as e:
        logger.error("Error parseando manifest.json: %s", e)
        return []
```

**Pasos para resolver:**
- [ ] A√±adir validaci√≥n de tipo (debe ser lista)
- [ ] Validar campos requeridos en cada entrada del manifest
- [ ] A√±adir logging claro cuando el manifest tiene estructura incorrecta
- [ ] A√±adir test que verifique validaci√≥n de manifest inv√°lido

---

### Issue 11: Funci√≥n `prepare_idealista_oferta` con l√≥gica incompleta

**Severidad:** üü° Media  
**Etiquetas:** `bug`, `etl`, `code-quality`, `priority-medium`

**Descripci√≥n:**

La funci√≥n `prepare_idealista_oferta` calcula `num_anuncios_tipologia` agrupando por tipolog√≠a pero no incluye este resultado en el DataFrame final. El resultado se asigna a `_` (descartado), lo que es c√≥digo muerto.

**Archivo afectado:**
- `src/etl/transformations/enrichment.py:277-283`

**C√≥digo problem√°tico:**
```python
if "tipologia" in df.columns:
    _ = (  # ‚ùå Resultado descartado
        df.groupby(group_cols + ["tipologia"])
        .size()
        .reset_index(name="num_anuncios_tipologia")
    )

aggregated = df.groupby(group_cols).agg(agg_dict).reset_index()
# num_anuncios_tipologia no se incluye en aggregated
```

**Impacto:**
- C√≥digo muerto que confunde a desarrolladores
- Informaci√≥n √∫til (distribuci√≥n por tipolog√≠a) se calcula pero no se usa
- Puede indicar funcionalidad incompleta o abandonada

**Soluci√≥n propuesta:**
- Opci√≥n 1: Eliminar el c√≥digo si no se necesita
- Opci√≥n 2: Incluir `num_anuncios_tipologia` en el resultado agregado si es √∫til

**Pasos para resolver:**
- [ ] Evaluar si `num_anuncios_tipologia` es informaci√≥n √∫til para an√°lisis
- [ ] Si es √∫til: incluir en el DataFrame agregado (pivot o agregaci√≥n adicional)
- [ ] Si no es √∫til: eliminar el c√≥digo muerto
- [ ] Documentar decisi√≥n en comentario o docstring

---

## Issues Ya Documentados (Referencias)

Los siguientes issues ya est√°n documentados en `docs/CODE_AUDIT_ISSUES.md` y no requieren nuevas issues:

- **Issue 5 del plan:** Hardcoding de a√±o 2022 en `main.py` ‚Üí Ver CODE_AUDIT_ISSUES.md #7 y #15
- **Issue 6 del plan:** Falta validaci√≥n de integridad referencial ‚Üí Ver CODE_AUDIT_ISSUES.md #8
- **Issue 10 del plan:** L√≥gica de deduplicaci√≥n sin documentaci√≥n ‚Üí Ver CODE_AUDIT_ISSUES.md #22
- **Issue 11 del plan:** Falta manejo de encoding fallback ‚Üí Ver CODE_AUDIT_ISSUES.md #23

## Issues Relacionadas en GitHub

Los siguientes issues existentes en GitHub est√°n relacionadas con algunos de los problemas identificados:

- **Issue #13:** "Fix: Deduplicaci√≥n agresiva en fact_precios" ‚Üí Relacionada con Issue 4 (validaci√≥n de pipes)
- **Issue #14:** "Feature: Completar campos demogr√°ficos faltantes" ‚Üí Relacionada con enriquecimiento de datos
- **Issue #15:** "Improvement: Mejorar mapeo de territorios Portal de Dades" ‚Üí Relacionada con Issue 7 (validaci√≥n de a√±os)
- **Issue #20:** "Task: Testing - Unit e Integration Tests" ‚Üí Relacionada con Issue 5 (tests marcados como skip)
- **Issue #40, #37:** "Tests de integraci√≥n para pipeline ETL" ‚Üí Relacionada con Issue 5
- **Issue #43:** "Refactor: Limpiar orquestador Pipeline" ‚Üí Relacionada con Issue 6 (manejo de errores)

---

## Resumen de Priorizaci√≥n

### üî¥ Alta Prioridad
- Ninguno (todos los cr√≠ticos ya est√°n documentados)

### üü° Media Prioridad
1. Issue 1: Bug en regex de `_parse_household_size`
2. Issue 2: Manejo de errores gen√©rico en `enrichment.py`
3. Issue 4: Validaci√≥n faltante en `prepare_fact_precios`
4. Issue 5: Tests marcados como skip
5. Issue 6: Manejo de errores silencioso en `pipeline.py`
6. Issue 7: Falta validaci√≥n de a√±os en Portal de Dades
7. Issue 10: Falta validaci√≥n de estructura de manifest.json
8. Issue 11: Funci√≥n `prepare_idealista_oferta` con l√≥gica incompleta

### üü¢ Baja Prioridad
1. Issue 3: Import no utilizado en `enrichment.py`
2. Issue 8: Workflow dashboard-demo sin validaci√≥n de puerto
3. Issue 9: Workflow kpi-update con manejo de errores gen√©rico

---

## Pr√≥ximos Pasos

1. Crear GitHub Issues para cada issue nuevo usando `gh issue create`
2. Asignar etiquetas y prioridades seg√∫n la clasificaci√≥n
3. Referenciar issues relacionadas cuando aplique
4. Actualizar este documento cuando los issues sean creados

