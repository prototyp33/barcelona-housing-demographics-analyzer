# Plan de ImplementaciÃ³n - Arquitectura de Base de Datos

**Fecha**: 2025-12-14  
**Basado en**: `docs/spike/DATABASE_ARCHITECTURE_DESIGN.md`

---

## ğŸ“Š Resumen Ejecutivo

Este plan prioriza la implementaciÃ³n de mejoras a la arquitectura de base de datos en 3 fases:

- **Fase 1 (Corto Plazo)**: Mejoras incrementales a tablas existentes
- **Fase 2 (Medio Plazo)**: Nuevas dimensiones y tablas de hechos
- **Fase 3 (Largo Plazo)**: Integraciones avanzadas y optimizaciones

---

## ğŸ¯ Fase 1: Mejoras Incrementales (Corto Plazo)

**DuraciÃ³n estimada**: 1-2 semanas  
**Prioridad**: ğŸ”´ Alta  
**Impacto**: Mejora inmediata sin romper compatibilidad

### 1.1 Mejorar `dim_barrios` con campos adicionales

**Objetivo**: AÃ±adir campos para matching y anÃ¡lisis geogrÃ¡fico.

**Campos a aÃ±adir**:
- `codigo_ine` TEXT - CÃ³digo INE para matching
- `centroide_lat` REAL - Latitud del centroide
- `centroide_lon` REAL - Longitud del centroide
- `area_km2` REAL - Ãrea en kmÂ²

**ImplementaciÃ³n**:
1. Crear migraciÃ³n SQL para aÃ±adir columnas
2. Actualizar `src/database_setup.py` con nuevos campos
3. Script para calcular centroides desde `geometry_json`
4. Script para calcular Ã¡reas desde geometrÃ­as
5. Actualizar ETL para poblar nuevos campos

**EstimaciÃ³n**: 4-6 horas

---

### 1.2 Crear tabla `dim_tiempo`

**Objetivo**: Tabla de tiempo para anÃ¡lisis temporal y normalizaciÃ³n.

**ImplementaciÃ³n**:
1. Crear tabla `dim_tiempo` con perÃ­odos 2015-2024
2. Generar registros para todos los perÃ­odos (anual, quarterly, mensual)
3. Actualizar `src/database_setup.py`
4. Crear script de poblaciÃ³n inicial
5. Actualizar queries de ejemplo para usar `dim_tiempo`

**EstimaciÃ³n**: 3-4 horas

---

### 1.3 Crear vistas analÃ­ticas bÃ¡sicas

**Objetivo**: Vistas SQL para anÃ¡lisis comunes.

**Vistas a crear**:
1. `v_affordability_quarterly` - Affordability por trimestre
2. `v_precios_evolucion_anual` - EvoluciÃ³n anual de precios
3. `v_demografia_resumen` - Resumen demogrÃ¡fico

**ImplementaciÃ³n**:
1. Crear archivo SQL con definiciones de vistas
2. Script para crear vistas en la BD
3. Documentar uso de cada vista
4. Tests para validar vistas

**EstimaciÃ³n**: 2-3 horas

---

## ğŸš€ Fase 2: Nuevas Dimensiones y Hechos (Medio Plazo)

**DuraciÃ³n estimada**: 3-4 semanas  
**Prioridad**: ğŸŸ¡ Media  
**Impacto**: Nuevas capacidades analÃ­ticas

### 2.1 Crear `dim_servicios` y `fact_proximidad`

**Objetivo**: AnÃ¡lisis de proximidad a servicios y POIs.

**ImplementaciÃ³n**:
1. DiseÃ±ar esquema de `dim_servicios`
2. Integrar con Google Maps API / Overpass OSM
3. Crear extractor de servicios
4. Calcular mÃ©tricas de proximidad
5. Crear `fact_proximidad` con agregaciones
6. Crear vista `v_barrios_mejor_conectados`

**EstimaciÃ³n**: 8-12 horas

**Dependencias**:
- API keys de Google Maps (si se usa)
- Acceso a Overpass API (OSM)

---

### 2.2 Crear `dim_fuentes_datos`

**Objetivo**: CatÃ¡logo de fuentes para trazabilidad.

**ImplementaciÃ³n**:
1. Crear tabla `dim_fuentes_datos`
2. Poblar con fuentes actuales
3. Actualizar ETL para registrar fuente en cada carga
4. Crear vista de calidad por fuente

**EstimaciÃ³n**: 2-3 horas

---

### 2.3 Implementar Framework de Data Quality

**Objetivo**: Sistema automatizado de validaciÃ³n de calidad.

**ImplementaciÃ³n**:
1. Crear clase `DataQualityChecker` en `src/quality/`
2. Implementar checks: completitud, validez, unicidad
3. Integrar en pipeline ETL
4. Generar reportes de calidad
5. Crear dashboard de calidad (opcional)

**EstimaciÃ³n**: 6-8 horas

---

## ğŸ”® Fase 3: Integraciones Avanzadas (Largo Plazo)

**DuraciÃ³n estimada**: 2-3 meses  
**Prioridad**: ğŸŸ¢ Baja  
**Impacto**: Capacidades avanzadas

### 3.1 IntegraciÃ³n con Catastro API

**Objetivo**: Datos de edificios y uso del suelo.

**ImplementaciÃ³n**:
1. Investigar API del Catastro
2. Crear extractor de Catastro
3. Crear tabla `fact_catastro`
4. Integrar en pipeline ETL

**EstimaciÃ³n**: 12-16 horas

**Dependencias**:
- Acceso a API del Catastro
- DocumentaciÃ³n de la API

---

### 3.2 Sistema de AuditorÃ­a

**Objetivo**: Tracking de cambios en datos crÃ­ticos.

**ImplementaciÃ³n**:
1. Crear tabla `audit_housing_changes`
2. Crear triggers para capturar cambios
3. Sistema de versionado de datos
4. Dashboard de auditorÃ­a

**EstimaciÃ³n**: 6-8 horas

---

### 3.3 MigraciÃ³n a PostgreSQL + PostGIS (Opcional)

**Objetivo**: Mejor soporte geoespacial y escalabilidad.

**ImplementaciÃ³n**:
1. DiseÃ±ar esquema PostgreSQL
2. Scripts de migraciÃ³n desde SQLite
3. Actualizar cÃ³digo para PostgreSQL
4. Tests de migraciÃ³n
5. DocumentaciÃ³n de migraciÃ³n

**EstimaciÃ³n**: 20-30 horas

**Nota**: Solo si se requiere escalabilidad o funcionalidades PostGIS avanzadas.

---

## ğŸ“‹ Issues a Crear

### Fase 1 (Corto Plazo)

1. **[FEAT] Mejorar dim_barrios con campos adicionales**
   - AÃ±adir codigo_ine, centroide, area_km2
   - Prioridad: ğŸ”´ Alta
   - EstimaciÃ³n: 4-6 horas

2. **[FEAT] Crear tabla dim_tiempo**
   - Tabla de tiempo para anÃ¡lisis temporal
   - Prioridad: ğŸ”´ Alta
   - EstimaciÃ³n: 3-4 horas

3. **[FEAT] Crear vistas analÃ­ticas bÃ¡sicas**
   - 3 vistas SQL para anÃ¡lisis comunes
   - Prioridad: ğŸŸ¡ Media
   - EstimaciÃ³n: 2-3 horas

### Fase 2 (Medio Plazo)

4. **[FEAT] Integrar dim_servicios y fact_proximidad**
   - AnÃ¡lisis de proximidad a servicios
   - Prioridad: ğŸŸ¡ Media
   - EstimaciÃ³n: 8-12 horas

5. **[FEAT] Crear dim_fuentes_datos**
   - CatÃ¡logo de fuentes para trazabilidad
   - Prioridad: ğŸŸ¢ Baja
   - EstimaciÃ³n: 2-3 horas

6. **[FEAT] Implementar Framework de Data Quality**
   - Sistema automatizado de validaciÃ³n
   - Prioridad: ğŸŸ¡ Media
   - EstimaciÃ³n: 6-8 horas

### Fase 3 (Largo Plazo)

7. **[FEAT] IntegraciÃ³n con Catastro API**
   - Datos de edificios y uso del suelo
   - Prioridad: ğŸŸ¢ Baja
   - EstimaciÃ³n: 12-16 horas

8. **[FEAT] Sistema de AuditorÃ­a de Datos**
   - Tracking de cambios en datos crÃ­ticos
   - Prioridad: ğŸŸ¢ Baja
   - EstimaciÃ³n: 6-8 horas

---

## ğŸ¯ MÃ©tricas de Ã‰xito

### Fase 1
- âœ… `dim_barrios` con 4 campos adicionales poblados
- âœ… `dim_tiempo` con perÃ­odos 2015-2024
- âœ… 3 vistas analÃ­ticas funcionando

### Fase 2
- âœ… `dim_servicios` con >100 servicios catalogados
- âœ… `fact_proximidad` con mÃ©tricas para 73 barrios
- âœ… Framework DQ generando reportes automÃ¡ticos

### Fase 3
- âœ… `fact_catastro` con datos de edificios
- âœ… Sistema de auditorÃ­a capturando cambios
- âœ… (Opcional) MigraciÃ³n a PostgreSQL completada

---

## ğŸ“… Timeline Sugerido

### Semana 1-2: Fase 1
- DÃ­a 1-2: Mejorar `dim_barrios`
- DÃ­a 3-4: Crear `dim_tiempo`
- DÃ­a 5: Crear vistas analÃ­ticas

### Semana 3-6: Fase 2
- Semana 3-4: `dim_servicios` y `fact_proximidad`
- Semana 5: `dim_fuentes_datos`
- Semana 6: Framework DQ

### Mes 2-3: Fase 3
- Mes 2: IntegraciÃ³n Catastro
- Mes 3: Sistema de auditorÃ­a (y migraciÃ³n PostgreSQL si aplica)

---

## ğŸ”— Dependencias

### Fase 1
- âœ… Ninguna (mejoras incrementales)

### Fase 2
- âš ï¸ API keys de Google Maps (opcional)
- âš ï¸ Acceso a Overpass API (OSM)

### Fase 3
- âš ï¸ Acceso a API del Catastro
- âš ï¸ DecisiÃ³n sobre migraciÃ³n PostgreSQL

---

## ğŸ“š Referencias

- **Arquitectura**: `docs/spike/DATABASE_ARCHITECTURE_DESIGN.md`
- **Estado Actual**: `src/database_setup.py`
- **ETL Pipeline**: `src/etl/pipeline.py`
- **Master Table**: `docs/spike/IMPLEMENTATION_SUMMARY.md`

---

**Estado**: âœ… Plan creado  
**Siguiente**: Crear issues en GitHub y comenzar Fase 1

