# Alternativa: Web Scraping Idealista (Sin API)

**Fecha**: 2025-12-19  
**Issue**: #202 - Fase 2  
**Referencia**: [Octoparse - C√≥mo extraer datos de Idealista](https://www.octoparse.es/blog/como-extraer-los-datos-de-idealista-con-web-scraping)

---

## üéØ Contexto

Seg√∫n el art√≠culo de Octoparse, la **API oficial de Idealista suele dar muchos errores y es muy limitada**. Esto explica por qu√© estamos teniendo dificultades con las credenciales API.

**Alternativa viable**: Web scraping con BeautifulSoup (m√°s simple que Playwright)

---

## ‚öñÔ∏è Legalidad del Scraping

Seg√∫n el art√≠culo:

> **Es legal scrapear los datos p√∫blicos de Idealista.com**; es perfectamente legal y √©tico rastrear los datos de Idealista.com de forma lenta y razonable.

**Consideraciones**:
- ‚úÖ Datos p√∫blicos son legales de scrapear
- ‚ö†Ô∏è Respetar GDPR al capturar datos personales (nombres, tel√©fonos)
- ‚ö†Ô∏è Hacer scraping de forma lenta y razonable (evitar bloqueos)
- ‚ö†Ô∏è Cumplir con t√©rminos de servicio de Idealista

---

## üîß Implementaci√≥n con BeautifulSoup

### **Ventajas vs Playwright**

| Aspecto | Playwright | BeautifulSoup |
|---------|------------|---------------|
| **Complejidad** | Alta (navegador completo) | Baja (solo parsing HTML) |
| **Detecci√≥n anti-bot** | Alta (detecta automatizaci√≥n) | Baja (solo requests HTTP) |
| **Velocidad** | Lenta (carga JS completo) | R√°pida (solo HTML) |
| **Memoria** | Alta | Baja |
| **Mantenimiento** | Alto (estructura JS cambia) | Medio (estructura HTML cambia) |

### **C√≥digo Base (del art√≠culo)**

```python
import requests
from bs4 import BeautifulSoup
import time
import pandas as pd

def scrape_idealista_gracia(max_pages: int = 3) -> pd.DataFrame:
    """
    Scrapea datos de Idealista para Gr√†cia usando BeautifulSoup.
    
    Args:
        max_pages: N√∫mero m√°ximo de p√°ginas a scrapear
        
    Returns:
        DataFrame con propiedades extra√≠das
    """
    base_url = 'https://www.idealista.com/venta-viviendas/barcelona/gracia/'
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    properties = []
    
    for page in range(1, max_pages + 1):
        url = f"{base_url}?pagina={page}"
        
        try:
            response = requests.get(url, headers=headers)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Buscar listings (estructura puede variar)
                listings = soup.find_all('article', class_='item')
                
                for listing in listings:
                    try:
                        # Extraer datos (ajustar selectores seg√∫n estructura real)
                        title_elem = listing.find('a', class_='item-link')
                        price_elem = listing.find('span', class_='item-price')
                        location_elem = listing.find('span', class_='item-detail')
                        
                        if title_elem and price_elem:
                            properties.append({
                                'title': title_elem.get_text(strip=True),
                                'price': price_elem.get_text(strip=True),
                                'location': location_elem.get_text(strip=True) if location_elem else '',
                                'url': title_elem.get('href', '') if title_elem else '',
                            })
                    except Exception as e:
                        logger.warning(f"Error extrayendo listing: {e}")
                        continue
                
                # Delay entre p√°ginas (importante para evitar bloqueos)
                time.sleep(3)  # 3 segundos entre p√°ginas
                
            else:
                logger.warning(f"Error HTTP {response.status_code} en p√°gina {page}")
                
        except Exception as e:
            logger.error(f"Error scrapeando p√°gina {page}: {e}")
            continue
    
    return pd.DataFrame(properties)
```

---

## üìã Plan de Implementaci√≥n

### **Paso 1: Crear Script BeautifulSoup**

**Archivo**: `spike-data-validation/scripts/fase2/scrape_idealista_beautifulsoup.py`

**Caracter√≠sticas**:
- ‚úÖ Usar `requests` + `BeautifulSoup` (m√°s simple que Playwright)
- ‚úÖ Headers realistas para evitar detecci√≥n
- ‚úÖ Delays entre requests (3-5 segundos)
- ‚úÖ Manejo de errores robusto
- ‚úÖ Extracci√≥n de campos clave: precio, superficie, direcci√≥n, URL

### **Paso 2: Probar con P√°gina de Prueba**

```bash
# Test con una sola p√°gina
python3 spike-data-validation/scripts/fase2/scrape_idealista_beautifulsoup.py \
  --max-pages 1 \
  --test-mode
```

### **Paso 3: Ajustar Selectores**

- Inspeccionar HTML real de Idealista
- Ajustar selectores CSS seg√∫n estructura actual
- Validar que se extraen todos los campos necesarios

### **Paso 4: Ejecutar Extracci√≥n Completa**

```bash
# Extracci√≥n completa (3-5 p√°ginas)
python3 spike-data-validation/scripts/fase2/scrape_idealista_beautifulsoup.py \
  --max-pages 5 \
  --output spike-data-validation/data/processed/fase2/idealista_gracia_scraped.csv
```

---

## ‚ö†Ô∏è Consideraciones Importantes

### **1. Estructura HTML Puede Cambiar**

> **Nota del art√≠culo**: La estructura HTML de la p√°gina web puede cambiar, por lo que es posible que tenga que ajustar los par√°metros `find` o `find_all` en consecuencia.

**Soluci√≥n**: 
- Validar selectores antes de ejecuci√≥n completa
- Usar m√∫ltiples selectores como fallback
- Logging detallado para debugging

### **2. Evitar Bloqueos**

**Recomendaciones del art√≠culo**:
- ‚úÖ Delays entre requests (3-5 segundos m√≠nimo)
- ‚úÖ Headers realistas (User-Agent de navegador real)
- ‚úÖ No hacer scraping frecuente o a gran escala
- ‚úÖ Respetar t√©rminos de servicio

### **3. Cumplimiento Legal**

- ‚úÖ Solo datos p√∫blicos (no datos personales protegidos)
- ‚úÖ Scraping lento y razonable
- ‚úÖ No sobrecargar servidores
- ‚ö†Ô∏è Revisar t√©rminos de servicio de Idealista

---

## üîÑ Comparaci√≥n de M√©todos

| M√©todo | Estado | Ventajas | Desventajas |
|--------|--------|----------|-------------|
| **API Oficial** | ‚è≥ Requiere credenciales | Datos estructurados, oficial | Errores frecuentes, limitado |
| **Playwright** | ‚ùå Bloqueado (Cloudflare) | JavaScript completo | Detectado como bot, lento |
| **BeautifulSoup** | ‚úÖ **RECOMENDADO** | Simple, r√°pido, menos detecci√≥n | HTML puede cambiar |

---

## üöÄ Pr√≥ximos Pasos

### **Opci√≥n A: Implementar BeautifulSoup** (Recomendado)

1. ‚úÖ Crear script `scrape_idealista_beautifulsoup.py`
2. ‚úÖ Probar con p√°gina de prueba
3. ‚úÖ Ajustar selectores seg√∫n HTML real
4. ‚úÖ Ejecutar extracci√≥n completa
5. ‚úÖ Matching con Catastro
6. ‚úÖ Re-entrenar modelo

### **Opci√≥n B: Continuar con API** (Si llegan credenciales)

1. ‚è≥ Esperar credenciales API
2. ‚è≥ Ejecutar `extract_idealista_api_gracia.py`
3. ‚è≥ Validar que funciona (puede tener errores seg√∫n art√≠culo)

### **Opci√≥n C: H√≠brido**

1. ‚úÖ Intentar API primero (si hay credenciales)
2. ‚úÖ Si falla, usar BeautifulSoup como fallback
3. ‚úÖ Combinar resultados si ambos funcionan

---

## üìù Notas del Art√≠culo

> **Sobre la API**: "Aunque idealista dispone de API para acceder a los datos, suele dar muchos errores de respuesta y es muy limitado."

**Implicaci√≥n**: Incluso con credenciales, la API puede no ser confiable. BeautifulSoup es una alternativa m√°s robusta.

---

## üîó Referencias

- [Art√≠culo Octoparse](https://www.octoparse.es/blog/como-extraer-los-datos-de-idealista-con-web-scraping)
- Script actual: `spike-data-validation/scripts/fase2/scrape_idealista_gracia.py` (Playwright)
- Script API: `spike-data-validation/scripts/fase2/extract_idealista_api_gracia.py`

---

**√öltima actualizaci√≥n**: 2025-12-19

