#!/usr/bin/env python3
"""
Scraping Idealista para Gr√†cia (Fase 2 - Issue #202).

Extrae anuncios de venta de Gr√†cia usando scraping web controlado.
Objetivo: 50-100 anuncios para matching con datos Catastro.

‚ö†Ô∏è IMPORTANTE: Scraping √©tico con delays largos para evitar bloqueos.
Idealista tiene medidas anti-bot muy agresivas.

Uso:
    python3 spike-data-validation/scripts/fase2/scrape_idealista_gracia.py
"""

from __future__ import annotations

import argparse
import json
import logging
import random
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import pandas as pd
from playwright.sync_api import Page, TimeoutError as PlaywrightTimeoutError, sync_playwright

logger = logging.getLogger(__name__)

# Configuraci√≥n
BASE_URL = "https://www.idealista.com"
GRACIA_SEARCH_URL = f"{BASE_URL}/venta-viviendas/barcelona/gracia/"
OUTPUT_DIR = Path("spike-data-validation/data/processed/fase2")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Delays √©ticos (segundos)
DELAY_BETWEEN_PAGES = 5.0  # Delay entre p√°ginas
DELAY_BETWEEN_REQUESTS = 2.0  # Delay entre requests
RANDOM_DELAY_RANGE = (1.0, 3.0)  # Delay aleatorio adicional


def setup_logging() -> None:
    """Configura logging."""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )


def random_delay(min_sec: float = 1.0, max_sec: float = 3.0) -> None:
    """Espera aleatoria para simular comportamiento humano."""
    delay = random.uniform(min_sec, max_sec)
    time.sleep(delay)


def extract_property_data(page: Page, property_url: str) -> Optional[Dict[str, Any]]:
    """
    Extrae datos de un anuncio individual.
    
    Args:
        page: P√°gina de Playwright
        property_url: URL del anuncio
        
    Returns:
        Diccionario con datos del anuncio o None si falla
    """
    try:
        logger.debug("Extrayendo: %s", property_url)
        page.goto(property_url, wait_until="networkidle", timeout=30000)
        random_delay(*RANDOM_DELAY_RANGE)

        # Extraer datos b√°sicos
        data: Dict[str, Any] = {
            "url": property_url,
            "timestamp": datetime.now().isoformat(),
        }

        # Precio
        try:
            precio_elem = page.query_selector('span[class*="price"]')
            if precio_elem:
                precio_text = precio_elem.inner_text()
                # Limpiar y convertir: "350.000 ‚Ç¨" -> 350000
                precio_clean = precio_text.replace(".", "").replace("‚Ç¨", "").replace(",", ".").strip()
                try:
                    data["precio"] = float(precio_clean)
                except ValueError:
                    data["precio"] = None
        except Exception:
            data["precio"] = None

        # Superficie
        try:
            superficie_elem = page.query_selector('span:has-text("m¬≤")')
            if superficie_elem:
                superficie_text = superficie_elem.inner_text()
                # Extraer n√∫mero: "85 m¬≤" -> 85
                import re
                match = re.search(r"(\d+(?:[.,]\d+)?)", superficie_text)
                if match:
                    data["superficie_m2"] = float(match.group(1).replace(",", "."))
        except Exception:
            data["superficie_m2"] = None

        # Habitaciones
        try:
            habitaciones_elem = page.query_selector('span:has-text("hab")')
            if habitaciones_elem:
                habitaciones_text = habitaciones_elem.inner_text()
                match = re.search(r"(\d+)", habitaciones_text)
                if match:
                    data["habitaciones"] = int(match.group(1))
        except Exception:
            data["habitaciones"] = None

        # Direcci√≥n
        try:
            direccion_elem = page.query_selector('h1[class*="title"], div[class*="address"]')
            if direccion_elem:
                data["direccion"] = direccion_elem.inner_text().strip()
        except Exception:
            data["direccion"] = None

        # Descripci√≥n (primeros 200 caracteres)
        try:
            desc_elem = page.query_selector('div[class*="description"]')
            if desc_elem:
                desc_text = desc_elem.inner_text().strip()
                data["descripcion"] = desc_text[:200] if len(desc_text) > 200 else desc_text
        except Exception:
            data["descripcion"] = None

        return data

    except PlaywrightTimeoutError:
        logger.warning("Timeout al cargar: %s", property_url)
        return None
    except Exception as exc:
        logger.warning("Error extrayendo %s: %s", property_url, exc)
        return None


def scrape_idealista_gracia(max_properties: int = 100, max_pages: int = 5) -> pd.DataFrame:
    """
    Scraping principal de Idealista para Gr√†cia.
    
    Args:
        max_properties: M√°ximo de propiedades a extraer
        max_pages: M√°ximo de p√°ginas a procesar
        
    Returns:
        DataFrame con propiedades extra√≠das
    """
    logger.info("=" * 70)
    logger.info("SCRAPING IDEALISTA - GR√ÄCIA")
    logger.info("=" * 70)
    logger.info("URL: %s", GRACIA_SEARCH_URL)
    logger.info("M√°ximo propiedades: %s", max_properties)
    logger.info("M√°ximo p√°ginas: %s", max_pages)
    logger.info("")

    properties: List[Dict[str, Any]] = []

    with sync_playwright() as p:
        # Iniciar navegador con configuraci√≥n anti-detecci√≥n
        browser = p.chromium.launch(
            headless=False,  # Cambiar a True en producci√≥n, False para debugging
            args=[
                "--disable-blink-features=AutomationControlled",
                "--disable-dev-shm-usage",
            ],
        )

        context = browser.new_context(
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            viewport={"width": 1920, "height": 1080},
            locale="es-ES",
            timezone_id="Europe/Madrid",
        )
        
        # A√±adir scripts para evitar detecci√≥n
        context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)

        page = context.new_page()

        try:
            # Navegar a p√°gina de b√∫squeda
            logger.info("Cargando p√°gina de b√∫squeda...")
            page.goto(GRACIA_SEARCH_URL, wait_until="domcontentloaded", timeout=30000)
            
            # Esperar m√°s tiempo para que se cargue el contenido din√°mico
            logger.info("Esperando carga completa (puede tardar por protecci√≥n anti-bot)...")
            random_delay(5, 8)  # Esperar m√°s tiempo inicial
            
            # Verificar si hay p√°gina de protecci√≥n
            page_title = page.title()
            page_content = page.content()
            
            if "cloudflare" in page_content.lower() or "checking your browser" in page_content.lower():
                logger.warning("‚ö†Ô∏è  Detectada p√°gina de protecci√≥n Cloudflare")
                logger.info("Esperando 10 segundos adicionales...")
                random_delay(10, 15)
                page.reload(wait_until="domcontentloaded", timeout=30000)
                random_delay(5, 8)
            
            # Verificar que la p√°gina se carg√≥ correctamente
            if "idealista" not in page_title.lower() and "idealista" not in page_content.lower()[:500]:
                logger.error("‚ùå La p√°gina no se carg√≥ correctamente. Posible bloqueo anti-bot.")
                logger.error("T√≠tulo de p√°gina: %s", page_title)
                logger.error("Considera usar la API oficial de Idealista o aumentar delays")
                return pd.DataFrame()

            # Aceptar cookies si aparece
            try:
                cookie_button = page.query_selector('button:has-text("Aceptar"), button:has-text("Accept")')
                if cookie_button:
                    cookie_button.click()
                    random_delay(1, 2)
            except Exception:
                pass

            # Esperar a que se cargue el contenido de propiedades
            logger.info("Esperando a que se cargue el contenido de propiedades...")
            try:
                # Esperar a que aparezcan elementos de propiedades (varios selectores posibles)
                # Intentar m√∫ltiples selectores con m√°s tiempo
                selectors_to_wait = [
                    'article',
                    '.item',
                    '[data-id]',
                    '.property-item',
                    'a[href*="/inmueble/"]',
                    'a[href*="/vivienda/"]',
                    '.detail-link',
                    '[class*="property"]',
                ]
                
                found_content = False
                for selector in selectors_to_wait:
                    try:
                        page.wait_for_selector(selector, timeout=15000, state="visible")
                        logger.info("   ‚úì Contenido encontrado con selector: %s", selector)
                        found_content = True
                        break
                    except PlaywrightTimeoutError:
                        continue
                
                if not found_content:
                    logger.warning("   ‚ö†Ô∏è  No se encontr√≥ contenido con selectores est√°ndar")
                    logger.info("   Esperando 5 segundos adicionales...")
                    random_delay(5, 7)
                
                random_delay(2, 3)
            except PlaywrightTimeoutError:
                logger.warning("Timeout esperando contenido. Continuando de todas formas...")

            # Extraer URLs de propiedades de cada p√°gina
            page_num = 1
            property_urls: List[str] = []

            while page_num <= max_pages and len(property_urls) < max_properties:
                logger.info("Procesando p√°gina %s...", page_num)

                # Extraer URLs de anuncios en esta p√°gina (m√∫ltiples estrategias)
                try:
                    # Estrategia 1: Buscar enlaces con /inmueble/ o /vivienda/
                    selectors = [
                        'a[href*="/inmueble/"]',
                        'a[href*="/vivienda/"]',
                        'article a',
                        '.item a',
                        '[data-id] a',
                    ]
                    
                    found_links = False
                    for selector in selectors:
                        try:
                            property_links = page.query_selector_all(selector)
                            logger.debug("   Selector '%s': %s enlaces encontrados", selector, len(property_links))
                            
                            for link in property_links:
                                href = link.get_attribute("href")
                                if href:
                                    # Normalizar URL
                                    if "/inmueble/" in href or "/vivienda/" in href:
                                        full_url = href if href.startswith("http") else f"{BASE_URL}{href}"
                                        if full_url not in property_urls:
                                            property_urls.append(full_url)
                                            found_links = True
                                            if len(property_urls) >= max_properties:
                                                break
                            
                            if found_links:
                                break
                        except Exception as exc:
                            logger.debug("   Selector '%s' fall√≥: %s", selector, exc)
                            continue
                    
                    if not found_links:
                        # Debug: guardar HTML de la p√°gina para inspecci√≥n
                        html_snippet = page.content()[:2000]
                        logger.warning("   No se encontraron enlaces. HTML snippet (primeros 2000 chars):")
                        logger.warning("   %s", html_snippet[:500])
                        
                except Exception as exc:
                    logger.warning("Error extrayendo URLs p√°gina %s: %s", page_num, exc)

                logger.info("   URLs encontradas hasta ahora: %s", len(property_urls))

                # Ir a siguiente p√°gina
                if page_num < max_pages and len(property_urls) < max_properties:
                    try:
                        # M√∫ltiples selectores para bot√≥n siguiente
                        next_selectors = [
                            'a[aria-label*="Siguiente"]',
                            'a:has-text("Siguiente")',
                            'a:has-text("Siguiente p√°gina")',
                            '.pagination a:has-text(">")',
                            'button:has-text("Siguiente")',
                        ]
                        
                        next_button = None
                        for selector in next_selectors:
                            try:
                                next_button = page.query_selector(selector)
                                if next_button:
                                    break
                            except Exception:
                                continue
                        
                        if next_button:
                            next_button.click()
                            random_delay(DELAY_BETWEEN_PAGES, DELAY_BETWEEN_PAGES + 2)
                            # Esperar a que se cargue nueva p√°gina
                            try:
                                page.wait_for_load_state("networkidle", timeout=10000)
                            except PlaywrightTimeoutError:
                                pass
                        else:
                            logger.info("No hay m√°s p√°ginas disponibles")
                            break
                    except Exception as exc:
                        logger.info("No se pudo ir a siguiente p√°gina: %s", exc)
                        break

                page_num += 1

            # Extraer datos de cada propiedad
            logger.info("")
            logger.info("Extrayendo datos de %s propiedades...", len(property_urls))

            for i, prop_url in enumerate(property_urls[:max_properties], 1):
                logger.info("[%s/%s] %s", i, min(len(property_urls), max_properties), prop_url)

                prop_data = extract_property_data(page, prop_url)
                if prop_data:
                    properties.append(prop_data)

                # Delay entre propiedades
                if i < len(property_urls):
                    random_delay(DELAY_BETWEEN_REQUESTS, DELAY_BETWEEN_REQUESTS + 2)

        finally:
            browser.close()

    # Convertir a DataFrame
    if properties:
        df = pd.DataFrame(properties)
        logger.info("")
        logger.info("‚úÖ Extracci√≥n completada: %s propiedades", len(df))
        return df
    else:
        logger.warning("‚ö†Ô∏è  No se extrajeron propiedades")
        return pd.DataFrame()


def main() -> int:
    """Punto de entrada principal."""
    setup_logging()
    parser = argparse.ArgumentParser(description="Scraping Idealista para Gr√†cia")
    parser.add_argument("--max-properties", type=int, default=100, help="M√°ximo propiedades a extraer")
    parser.add_argument("--max-pages", type=int, default=5, help="M√°ximo p√°ginas a procesar")
    parser.add_argument("--output", type=str, default=None, help="Ruta de salida CSV")
    args = parser.parse_args()

    # Scraping
    df = scrape_idealista_gracia(
        max_properties=args.max_properties,
        max_pages=args.max_pages,
    )

    if df.empty:
        logger.error("No se extrajeron datos")
        return 1

    # Guardar CSV
    output_path = Path(args.output) if args.output else OUTPUT_DIR / "idealista_gracia_scraped.csv"
    df.to_csv(output_path, index=False, encoding="utf-8")
    logger.info("")
    logger.info("üìÑ CSV guardado: %s", output_path)

    # Guardar metadata
    metadata = {
        "timestamp": datetime.now().isoformat(),
        "num_properties": len(df),
        "completitud": {
            "precio": float(df["precio"].notna().sum() / len(df) * 100) if "precio" in df.columns else 0,
            "superficie": float(df["superficie_m2"].notna().sum() / len(df) * 100) if "superficie_m2" in df.columns else 0,
            "direccion": float(df["direccion"].notna().sum() / len(df) * 100) if "direccion" in df.columns else 0,
        },
    }
    metadata_path = OUTPUT_DIR / "idealista_scraping_metadata.json"
    metadata_path.write_text(json.dumps(metadata, indent=2), encoding="utf-8")
    logger.info("üìÑ Metadata guardada: %s", metadata_path)

    return 0


if __name__ == "__main__":
    raise SystemExit(main())

