#!/usr/bin/env python3
"""
Scraping de Idealista usando BeautifulSoup (alternativa a API y Playwright).

Basado en: https://www.octoparse.es/blog/como-extraer-los-datos-de-idealista-con-web-scraping

Ventajas:
- M√°s simple que Playwright (no requiere navegador completo)
- Menos detecci√≥n anti-bot que Playwright
- M√°s r√°pido (solo parsing HTML, no ejecuta JavaScript)

Uso:
    python3 spike-data-validation/scripts/fase2/scrape_idealista_beautifulsoup.py \
        --max-pages 5 \
        --output idealista_gracia_scraped.csv
"""

from __future__ import annotations

import argparse
import logging
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

import pandas as pd
import requests
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

# URL base para Gr√†cia
BASE_URL = "https://www.idealista.com/venta-viviendas/barcelona/gracia/"

# Headers exactos del art√≠culo Octoparse (m√°s simples)
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}


def setup_logging() -> None:
    """Configura logging."""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )


def extract_property_data(listing: BeautifulSoup) -> Optional[Dict[str, Any]]:
    """
    Extrae datos de un listing individual.
    
    Usa selectores exactos del art√≠culo Octoparse primero.
    
    Args:
        listing: Elemento BeautifulSoup con un listing
        
    Returns:
        Diccionario con datos de la propiedad o None si falla
    """
    try:
        # Selectores exactos del art√≠culo Octoparse primero
        title_elem = listing.find('a', class_='item-link')
        price_elem = listing.find('span', class_='item-price')
        location_elem = listing.find('span', class_='item-detail')
        
        # Si no funcionan, intentar alternativos
        if not title_elem:
            title_elem = (
                listing.find('a', {'class': 'item-link'}) or
                listing.find('h2', class_='item-title') or
                listing.find('a', href=True)
            )
        
        if not price_elem:
            price_elem = (
                listing.find('div', class_='item-price') or
                listing.find('span', {'class': 'price'})
            )
        
        if not location_elem:
            location_elem = (
                listing.find('div', class_='item-detail') or
                listing.find('span', {'class': 'location'})
            )
        
        # Extraer superficie si est√° disponible
        surface_elem = (
            listing.find('span', class_='item-detail-surface') or
            listing.find('span', {'class': 'surface'})
        )
        
        # Extraer habitaciones si est√° disponible
        rooms_elem = (
            listing.find('span', class_='item-detail-rooms') or
            listing.find('span', {'class': 'rooms'})
        )
        
        if not title_elem or not price_elem:
            return None
        
        # Extraer URL
        url = title_elem.get('href', '')
        if url and not url.startswith('http'):
            url = f"https://www.idealista.com{url}"
        
        # Extraer texto
        title = title_elem.get_text(strip=True)
        price_text = price_elem.get_text(strip=True)
        location = location_elem.get_text(strip=True) if location_elem else ''
        surface = surface_elem.get_text(strip=True) if surface_elem else ''
        rooms = rooms_elem.get_text(strip=True) if rooms_elem else ''
        
        # Limpiar precio (remover s√≠mbolos, espacios)
        price_clean = price_text.replace('‚Ç¨', '').replace('.', '').replace(',', '').strip()
        try:
            price = int(price_clean) if price_clean else None
        except ValueError:
            price = None
        
        return {
            'title': title,
            'price': price,
            'price_text': price_text,
            'location': location,
            'surface': surface,
            'rooms': rooms,
            'url': url,
        }
        
    except Exception as e:
        logger.debug(f"Error extrayendo listing: {e}")
        return None


def scrape_page(url: str, page_num: int) -> List[Dict[str, Any]]:
    """
    Scrapea una p√°gina de resultados.
    
    Args:
        url: URL de la p√°gina
        page_num: N√∫mero de p√°gina (para logging)
        
    Returns:
        Lista de diccionarios con propiedades
    """
    properties = []
    
    try:
        logger.info(f"Scrapeando p√°gina {page_num}: {url}")
        
        response = requests.get(url, headers=HEADERS, timeout=10)
        
        if response.status_code != 200:
            logger.warning(f"Error HTTP {response.status_code} en p√°gina {page_num}")
            return properties
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Buscar listings (c√≥digo exacto del art√≠culo primero)
        listings = soup.find_all('article', class_='item')
        
        if not listings:
            # Intentar selectores alternativos si el principal falla
            listings = (
                soup.find_all('article') or
                soup.find_all('div', class_='item') or
                soup.find_all('article', {'class': 'item'}) or
                soup.find_all('div', {'class': 'item'}) or
                soup.find_all('div', {'data-adid': True})
            )
        
        logger.info(f"   Encontrados {len(listings)} listings en p√°gina {page_num}")
        
        for listing in listings:
            prop_data = extract_property_data(listing)
            if prop_data:
                prop_data['page'] = page_num
                properties.append(prop_data)
        
        logger.info(f"   Extra√≠das {len(properties)} propiedades v√°lidas")
        
    except requests.exceptions.RequestException as e:
        logger.error(f"Error de red en p√°gina {page_num}: {e}")
    except Exception as e:
        logger.error(f"Error inesperado en p√°gina {page_num}: {e}")
    
    return properties


def scrape_idealista_gracia(max_pages: int = 5, delay: float = 3.0) -> pd.DataFrame:
    """
    Scrapea datos de Idealista para Gr√†cia.
    
    Args:
        max_pages: N√∫mero m√°ximo de p√°ginas a scrapear
        delay: Delay entre p√°ginas (segundos)
        
    Returns:
        DataFrame con propiedades extra√≠das
    """
    logger.info("=" * 70)
    logger.info("SCRAPING IDEALISTA - GR√ÄCIA (BeautifulSoup)")
    logger.info("=" * 70)
    logger.info(f"URL base: {BASE_URL}")
    logger.info(f"M√°ximo p√°ginas: {max_pages}")
    logger.info(f"Delay entre p√°ginas: {delay}s")
    logger.info("")
    
    all_properties = []
    
    for page in range(1, max_pages + 1):
        # Construir URL de p√°gina
        if page == 1:
            url = BASE_URL
        else:
            url = f"{BASE_URL}?pagina={page}"
        
        # Scrapear p√°gina
        properties = scrape_page(url, page)
        all_properties.extend(properties)
        
        # Delay entre p√°ginas (importante para evitar bloqueos)
        if page < max_pages:
            logger.info(f"   Esperando {delay}s antes de siguiente p√°gina...")
            time.sleep(delay)
    
    logger.info("")
    logger.info(f"‚úÖ Total propiedades extra√≠das: {len(all_properties)}")
    
    if not all_properties:
        logger.warning("‚ö†Ô∏è  No se extrajeron propiedades. Verificar:")
        logger.warning("   1. Estructura HTML de Idealista puede haber cambiado")
        logger.warning("   2. Selectores CSS pueden necesitar ajuste")
        logger.warning("   3. Puede haber protecci√≥n anti-bot activa")
        return pd.DataFrame()
    
    df = pd.DataFrame(all_properties)
    return df


def main() -> int:
    """Punto de entrada principal."""
    setup_logging()
    
    parser = argparse.ArgumentParser(description="Scrapear Idealista con BeautifulSoup")
    parser.add_argument("--max-pages", type=int, default=5, help="M√°ximo p√°ginas a scrapear")
    parser.add_argument("--delay", type=float, default=3.0, help="Delay entre p√°ginas (segundos)")
    parser.add_argument("--output", type=str, default=None, help="Archivo CSV de salida")
    parser.add_argument("--test-mode", action="store_true", help="Modo test (solo 1 p√°gina)")
    args = parser.parse_args()
    
    if args.test_mode:
        args.max_pages = 1
        logger.info("üß™ Modo test activado (1 p√°gina)")
    
    # Scrapear
    df = scrape_idealista_gracia(max_pages=args.max_pages, delay=args.delay)
    
    if df.empty:
        logger.error("‚ùå No se pudieron extraer propiedades")
        logger.error("")
        logger.error("Posibles causas:")
        logger.error("   1. Estructura HTML de Idealista cambi√≥")
        logger.error("   2. Selectores CSS necesitan actualizaci√≥n")
        logger.error("   3. Protecci√≥n anti-bot activa")
        logger.error("")
        logger.error("Siguiente paso: Inspeccionar HTML manualmente y ajustar selectores")
        return 1
    
    # Guardar CSV
    output_path = Path(args.output) if args.output else Path("spike-data-validation/data/processed/fase2/idealista_gracia_scraped.csv")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    df.to_csv(output_path, index=False, encoding="utf-8")
    logger.info("")
    logger.info(f"üìÑ CSV guardado: {output_path}")
    logger.info(f"   Propiedades: {len(df)}")
    logger.info(f"   Columnas: {', '.join(df.columns)}")
    
    # Estad√≠sticas b√°sicas
    if 'price' in df.columns:
        prices = df['price'].dropna()
        if len(prices) > 0:
            logger.info("")
            logger.info("üìä Estad√≠sticas de precios:")
            logger.info(f"   Media: {prices.mean():.0f} ‚Ç¨")
            logger.info(f"   Mediana: {prices.median():.0f} ‚Ç¨")
            logger.info(f"   Min: {prices.min():.0f} ‚Ç¨")
            logger.info(f"   Max: {prices.max():.0f} ‚Ç¨")
    
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

