#!/usr/bin/env python3
"""
Entrena y eval√∫a MACRO v0.2 con features adicionales de renta y demograf√≠a.

Mejoras sobre v0.1:
- Features de renta: renta_promedio_barrio, renta_mediana_barrio
- Features demogr√°ficas: poblacion_total, proporciones por edad/sexo/nacionalidad

Uso:
    python3 spike-data-validation/scripts/train_macro_v02.py \
        --input spike-data-validation/data/processed/gracia_merged_agg_barrio_anio_dataset_v02.csv \
        --output spike-data-validation/data/logs/macro_model_v02.json
"""

from __future__ import annotations

import argparse
import json
import logging
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Tuple

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

logger = logging.getLogger(__name__)

# Rutas por defecto
DEFAULT_INPUT = Path("spike-data-validation/data/processed/gracia_merged_agg_barrio_anio_dataset_v02.csv")
DEFAULT_REPORT = Path("spike-data-validation/data/logs/macro_model_v02.json")
DEFAULT_PRED = Path("spike-data-validation/data/processed/macro_predictions_v02.csv")

LOG_DIR = Path("spike-data-validation/data/logs")
PROCESSED_DIR = Path("spike-data-validation/data/processed")


@dataclass(frozen=True)
class TrainConfig:
    """Configuraci√≥n del modelo MACRO v0.2."""
    seed: int = 42
    temporal_test_year: int = 2025


def setup_logging() -> None:
    """Configura logging."""
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )


def prepare_features(
    df: pd.DataFrame,
    include_renta: bool = True,
    include_demografia: bool = False  # Por defecto: NO incluir (no aportan valor)
) -> Tuple[pd.DataFrame, List[str]]:
    """
    Prepara features para el modelo MACRO v0.2.
    
    Args:
        df: DataFrame enriquecido
        include_renta: Incluir features de renta
        include_demografia: Incluir features demogr√°ficas (por defecto False)
        
    Returns:
        Tupla (df_clean, feature_names)
    """
    logger.info("Preparando features...")
    
    # Features estructurales (base)
    structural_features = [
        'superficie_m2_barrio_mean',
        'ano_construccion_barrio_mean',
        'plantas_barrio_mean'
    ]
    
    # Features de renta
    renta_features = [
        'renta_promedio_barrio',
        'renta_mediana_barrio',
    ]
    
    # Features demogr√°ficas (seleccionar las m√°s relevantes)
    # NOTA: Por defecto NO se incluyen porque no aportan valor (coeficientes ‚âà 0)
    demografia_features = [
        'poblacion_total',
        'prop_18_34',  # Poblaci√≥n joven (demanda alta)
        'prop_65_plus',  # Poblaci√≥n mayor (demanda baja)
        'prop_extranjeros',  # Diversidad (puede afectar precios)
    ]
    
    # Features temporales y categ√≥ricas
    other_features = ['anio']
    
    # Seleccionar features seg√∫n flags
    all_features = structural_features.copy()
    all_features.extend(other_features)
    
    if include_renta:
        all_features.extend(renta_features)
        logger.info("  ‚úÖ Incluyendo features de renta")
    else:
        logger.info("  ‚ö†Ô∏è  Excluyendo features de renta")
    
    if include_demografia:
        all_features.extend(demografia_features)
        logger.info("  ‚úÖ Incluyendo features demogr√°ficas")
    else:
        logger.info("  ‚ö†Ô∏è  Excluyendo features demogr√°ficas (no aportan valor)")
    
    # Filtrar features disponibles
    available_features = [f for f in all_features if f in df.columns]
    missing_features = set(all_features) - set(available_features)
    
    if missing_features:
        logger.warning(f"  ‚ö†Ô∏è  Features faltantes: {missing_features}")
    
    # Seleccionar columnas necesarias
    required_cols = ['precio_m2_mean'] + available_features
    if 'dataset_id' in df.columns:
        required_cols.append('dataset_id')
    
    # Filtrar filas con todas las features necesarias
    df_clean = df[required_cols].copy()
    
    # Para features num√©ricas, imputar con mediana si hay nulos
    numeric_features = [f for f in available_features if f != 'anio']
    for col in numeric_features:
        if df_clean[col].isna().any():
            median_val = df_clean[col].median()
            n_missing = df_clean[col].isna().sum()
            df_clean[col] = df_clean[col].fillna(median_val)
            logger.info(f"    Imputado {col}: {n_missing} valores con mediana ({median_val:.2f})")
    
    logger.info(f"  ‚úÖ Features preparadas: {len(available_features)} features")
    logger.info(f"     Observaciones: {len(df_clean)}")
    
    return df_clean, available_features


def build_design_matrix(
    df: pd.DataFrame,
    feature_names: List[str],
    use_dummies_dataset: bool = True,
    dataset_dummy_columns: List[str] = None
) -> Tuple[np.ndarray, List[str], pd.DataFrame, List[str]]:
    """
    Construye matriz de dise√±o X con features num√©ricas y dummies.
    
    Args:
        df: DataFrame con features
        feature_names: Lista de nombres de features num√©ricas
        use_dummies_dataset: Si True, a√±ade dummies de dataset_id
        dataset_dummy_columns: Columnas de dummies a usar (para consistencia train/test)
        
    Returns:
        Tupla (X, feature_names_full, df_base, dataset_dummy_cols_used)
    """
    df_base = df.copy()
    
    # Features num√©ricas
    X_parts: List[np.ndarray] = [np.ones(len(df_base))]  # Intercept
    names: List[str] = ["intercept"]
    
    for feat in feature_names:
        if feat in df_base.columns:
            X_parts.append(pd.to_numeric(df_base[feat], errors="coerce").to_numpy(dtype=float))
            names.append(feat)
    
    # Dummies de dataset_id (usar columnas predefinidas para consistencia)
    dataset_dummy_cols_used = []
    if use_dummies_dataset and 'dataset_id' in df_base.columns:
        if dataset_dummy_columns is None:
            # Primera vez: crear dummies
            dataset_dummies = pd.get_dummies(df_base['dataset_id'], prefix='dataset', drop_first=True)
            dataset_dummy_cols_used = dataset_dummies.columns.tolist()
        else:
            # Usar columnas predefinidas (para test)
            dataset_dummies = pd.get_dummies(df_base['dataset_id'], prefix='dataset', drop_first=True)
            # Asegurar que todas las columnas esperadas est√©n presentes
            for col in dataset_dummy_columns:
                if col in dataset_dummies.columns:
                    dataset_dummy_cols_used.append(col)
                else:
                    # A√±adir columna de ceros si no existe
                    dataset_dummies[col] = 0
                    dataset_dummy_cols_used.append(col)
            # Mantener solo las columnas esperadas
            dataset_dummies = dataset_dummies[dataset_dummy_columns]
        
        for col in dataset_dummy_cols_used:
            X_parts.append(dataset_dummies[col].to_numpy(dtype=float))
            names.append(col)
    
    X = np.column_stack(X_parts)
    
    return X, names, df_base, dataset_dummy_cols_used


def train_eval_temporal_split(
    df: pd.DataFrame,
    feature_names: List[str],
    config: TrainConfig,
    include_demografia: bool = False
) -> Tuple[Dict[str, Any], pd.DataFrame]:
    """
    Entrena y eval√∫a modelo con split temporal.
    
    Args:
        df: DataFrame con features
        feature_names: Lista de nombres de features
        config: Configuraci√≥n de entrenamiento
        
    Returns:
        Tupla (m√©tricas, predicciones)
    """
    logger.info("=" * 70)
    logger.info("ENTRENAMIENTO Y EVALUACI√ìN - MACRO v0.2")
    logger.info("=" * 70)
    
    # Split temporal
    train_mask = df['anio'] < config.temporal_test_year
    test_mask = df['anio'] == config.temporal_test_year
    
    df_train = df[train_mask].copy()
    df_test = df[test_mask].copy()
    
    logger.info(f"Train: {len(df_train)} observaciones ({df_train['anio'].min()}-{df_train['anio'].max()})")
    logger.info(f"Test: {len(df_test)} observaciones ({df_test['anio'].min()}-{df_test['anio'].max()})")
    
    if len(df_train) == 0 or len(df_test) == 0:
        logger.error("No hay suficientes datos para split temporal")
        return {}, pd.DataFrame()
    
    # Construir matrices de dise√±o (train primero para obtener columnas de dummies)
    X_train, feature_names_full, _, dataset_dummy_cols = build_design_matrix(df_train, feature_names)
    X_test, _, _, _ = build_design_matrix(df_test, feature_names, dataset_dummy_columns=dataset_dummy_cols)
    
    y_train = df_train['precio_m2_mean'].to_numpy()
    y_test = df_test['precio_m2_mean'].to_numpy()
    
    # Entrenar modelo
    logger.info("Entrenando modelo...")
    model = LinearRegression()
    model.fit(X_train, y_train)
    
    # Predicciones
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # M√©tricas
    train_metrics = {
        'r2': float(r2_score(y_train, y_train_pred)),
        'rmse': float(np.sqrt(mean_squared_error(y_train, y_train_pred))),
        'mae': float(mean_absolute_error(y_train, y_train_pred))
    }
    
    test_metrics = {
        'r2': float(r2_score(y_test, y_test_pred)),
        'rmse': float(np.sqrt(mean_squared_error(y_test, y_test_pred))),
        'mae': float(mean_absolute_error(y_test, y_test_pred))
    }
    
    # Bias (sesgo)
    test_bias = float(np.mean(y_test_pred - y_test))
    
    logger.info("\nüìä M√©tricas de Entrenamiento:")
    logger.info(f"   R¬≤: {train_metrics['r2']:.4f}")
    logger.info(f"   RMSE: {train_metrics['rmse']:.2f} ‚Ç¨/m¬≤")
    logger.info(f"   MAE: {train_metrics['mae']:.2f} ‚Ç¨/m¬≤")
    
    logger.info("\nüìä M√©tricas de Test (2025):")
    logger.info(f"   R¬≤: {test_metrics['r2']:.4f}")
    logger.info(f"   RMSE: {test_metrics['rmse']:.2f} ‚Ç¨/m¬≤")
    logger.info(f"   MAE: {test_metrics['mae']:.2f} ‚Ç¨/m¬≤")
    logger.info(f"   Bias: {test_bias:+.2f} ‚Ç¨/m¬≤")
    
    # Coeficientes
    coefficients = dict(zip(feature_names_full, model.coef_))
    intercept = float(model.intercept_)
    
    logger.info("\nüìä Coeficientes del Modelo:")
    logger.info(f"   Intercept: {intercept:.2f}")
    for feat, coef in sorted(coefficients.items()):
        if feat != 'intercept':
            logger.info(f"   {feat}: {coef:.4f}")
    
    # DataFrame de predicciones
    pred_data = {
        'anio': df_test['anio'].values,
        'precio_m2_mean_true': y_test,
        'precio_m2_mean_pred': y_test_pred,
        'residual': y_test - y_test_pred
    }
    
    # A√±adir barrio_id si est√° disponible
    if 'barrio_id' in df_test.columns:
        pred_data['barrio_id'] = df_test['barrio_id'].values
    
    # A√±adir dataset_id si est√° disponible
    if 'dataset_id' in df_test.columns:
        pred_data['dataset_id'] = df_test['dataset_id'].values
    
    pred_df = pd.DataFrame(pred_data)
    
    # M√©tricas completas
    metrics = {
        'model_version': 'MACRO v0.2' + ('_with_demo' if include_demografia else '_simplified'),
        'timestamp': datetime.now().isoformat(),
        'train': train_metrics,
        'test': test_metrics,
        'test_bias': test_bias,
        'n_train': len(df_train),
        'n_test': len(df_test),
        'features': feature_names_full,
        'coefficients': {k: float(v) for k, v in coefficients.items()},
        'intercept': intercept,
    }
    
    return metrics, pred_df


def main():
    """Funci√≥n principal."""
    parser = argparse.ArgumentParser(
        description="Entrena modelo MACRO v0.2 con features de renta y demograf√≠a"
    )
    parser.add_argument(
        "--input",
        type=Path,
        default=DEFAULT_INPUT,
        help="Archivo CSV del dataset MACRO v0.2 enriquecido"
    )
    parser.add_argument(
        "--report",
        type=Path,
        default=DEFAULT_REPORT,
        help="Archivo JSON de m√©tricas"
    )
    parser.add_argument(
        "--pred",
        type=Path,
        default=DEFAULT_PRED,
        help="Archivo CSV de predicciones"
    )
    parser.add_argument(
        "--no-renta",
        action="store_true",
        help="No incluir features de renta"
    )
    parser.add_argument(
        "--with-demo",
        action="store_true",
        help="Incluir features demogr√°ficas (por defecto NO se incluyen)"
    )
    
    args = parser.parse_args()
    setup_logging()
    
    # Cargar datos
    logger.info(f"Cargando dataset: {args.input}")
    if not args.input.exists():
        logger.error(f"Archivo no encontrado: {args.input}")
        return 1
    
    df = pd.read_csv(args.input)
    logger.info(f"  ‚úÖ {len(df)} observaciones cargadas")
    
    # Preparar features
    include_demo_flag = args.with_demo  # Por defecto False (no incluir)
    df_clean, feature_names = prepare_features(
        df,
        include_renta=not args.no_renta,
        include_demografia=include_demo_flag
    )
    
    # Entrenar y evaluar
    config = TrainConfig()
    metrics, pred_df = train_eval_temporal_split(df_clean, feature_names, config, include_demografia=include_demo_flag)
    
    if not metrics:
        return 1
    
    # Guardar m√©tricas
    args.report.parent.mkdir(parents=True, exist_ok=True)
    with open(args.report, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)
    logger.info(f"‚úÖ M√©tricas guardadas: {args.report}")
    
    # Guardar predicciones
    args.pred.parent.mkdir(parents=True, exist_ok=True)
    pred_df.to_csv(args.pred, index=False)
    logger.info(f"‚úÖ Predicciones guardadas: {args.pred}")
    
    logger.info("=" * 70)
    logger.info("‚úÖ Proceso completado")
    
    return 0


if __name__ == "__main__":
    exit(main())

