#!/usr/bin/env bash

set -euo pipefail

# Script para sincronizar issues t√©cnicas en GitHub siguiendo el estilo del proyecto.
# Nota: Ajusta --project y --milestone si es necesario antes de ejecutar.

current_user="@me"

#
# Issue 1: Integrar geometr√≠as de barrios en dim_barrios.geometry_json
#
gh issue create \
  --title "üìã [S1] Integrar geometr√≠as de barrios en dim_barrios.geometry_json" \
  --label "sprint-1" \
  --label "database" \
  --label "visualization" \
  --label "priority-high" \
  --assignee "${current_user}" \
  --body '
### üìå Objetivo

Integrar de forma robusta las geometr√≠as de los 73 barrios de Barcelona en la columna `geometry_json` de `dim_barrios`, garantizando que el pipeline ETL deje la tabla lista para visualizaciones geoespaciales (Mapbox, choropleths, etc.).

### üîç Descripci√≥n del Problema

Aunque existe el script `scripts/load_geometries.py` y `prepare_dim_barrios()` soporta un `geojson_path` opcional, el estado actual de la base de datos indica que `geometry_json` sigue en NULL para la mayor√≠a (o todos) los barrios. Esto implica que la carga de geometr√≠as no est√° integrada ni automatizada dentro del pipeline ETL principal, y no hay una verificaci√≥n sistem√°tica de cobertura (73/73 barrios) ni de validez de los GeoJSON cargados.

### üìù Pasos para Implementar

1. Definir fuente can√≥nica de geometr√≠as (archivo(s) GeoJSON en `data/raw/geometries/`), documentando su procedencia (Open Data BCN u otra).  
2. Conectar `scripts/load_geometries.py` con el pipeline principal (`scripts/process_and_load.py` o equivalente) para que la carga de geometr√≠as forme parte del ETL est√°ndar.  
3. Asegurar que `prepare_dim_barrios()` reciba `geojson_path` cuando existan geometr√≠as y que el mapeo `barrio_id` ‚Üî geometr√≠a sea consistente con `codi_barri`.  
4. Implementar verificaci√≥n posterior a la carga: consulta a `dim_barrios` para comprobar que 73/73 barrios tienen `geometry_json` no nulo.  
5. A√±adir logs detallados (INFO/WARNING) sobre barrios sin geometr√≠a, features saltadas y tipos de geometr√≠a inv√°lidos.  
6. Actualizar o crear notebook de validaci√≥n (p.ej. `04-eda-precios.ipynb` o uno nuevo) con un mapa simple que consuma `geometry_json` para validar visualmente.  
7. Documentar el flujo en `docs/DATA_STRUCTURE.md` y/o un documento espec√≠fico de geometr√≠as (fuentes, supuestos, limitaciones).

### ‚úÖ Definici√≥n de Hecho (Definition of Done)

- [ ] `dim_barrios.geometry_json` poblado para 73/73 barrios con GeoJSON v√°lido.  
- [ ] Pipeline ETL (`scripts/process_and_load.py`) ejecuta la carga de geometr√≠as sin pasos manuales adicionales.  
- [ ] Logs claros indicando n√∫mero de barrios actualizados, skippeados y con errores.  
- [ ] Existe un check automatizado (script o test) que falla si `geometry_json` est√° vac√≠o o incompleto.  
- [ ] Notebook de validaci√≥n con un mapa funcionando usando `geometry_json`.  
- [ ] Documentaci√≥n actualizada describiendo la fuente de geometr√≠as y el proceso de carga.

### üéØ Impacto & KPI

- **KPI afectado:** % de barrios con geometr√≠a v√°lida (objetivo: 100% = 73/73).  
- **Impacto directo:** Habilita mapas coropl√©ticos y an√°lisis espaciales (densidad, renta, precios) en notebooks y dashboard futuro.  
- **Riesgo mitigado:** Evitar inconsistencias entre nombres/c√≥digos de barrios y sus pol√≠gonos geogr√°ficos.

### üîó Issues Relacionadas

- Relacionada con: tareas de visualizaci√≥n y futuro dashboard Streamlit.  
- Conecta con: enriquecimiento de `fact_demografia` y `fact_precios` para mapas tem√°ticos.

### üöß Riesgos / Bloqueos

- Posibles discrepancias entre los nombres/c√≥digos de barrios en el GeoJSON y en `dim_barrios`.  
- Cambios futuros en la fuente de datos geogr√°ficos (nuevos l√≠mites administrativos).  
- Tama√±o de los GeoJSON y rendimiento en consultas si no se optimizan correctamente.

### üìö Enlaces Relevantes

- `scripts/load_geometries.py`  
- `src/transform/cleaners.py` (`prepare_dim_barrios` y helpers de GeoJSON)  
- `src/database_setup.py` (definici√≥n de `dim_barrios`)  
- `docs/PROJECT_STATUS.md` (secci√≥n Geometry JSON vac√≠o)  

### ‚è±Ô∏è Tiempo Estimado

**4-6 horas**
'


#
# Issue 2: Endurecer verificaci√≥n de integridad de datos (scripts/verify_integrity.py)
#
gh issue create \
  --title "üìã [S1] Endurecer verificaci√≥n de integridad de datos (verify_integrity.py)" \
  --label "sprint-1" \
  --label "testing" \
  --label "quality-assurance" \
  --label "etl" \
  --label "priority-medium" \
  --assignee "${current_user}" \
  --body '
### üìå Objetivo

Convertir `scripts/verify_integrity.py` en una herramienta robusta de verificaci√≥n de integridad de datos que cubra las tablas principales (`fact_precios`, `fact_demografia`, `fact_renta`, `fact_oferta_idealista`, `dim_barrios`) y se integre f√°cilmente en el flujo de desarrollo (CLI y CI).

### üîç Descripci√≥n del Problema

El script actual `scripts/verify_integrity.py` realiza algunas comprobaciones b√°sicas usando `print()` y se centra principalmente en duplicados de `fact_precios` y nulls simples en `fact_demografia`. No sigue los est√°ndares de logging del proyecto, no valida tablas nuevas (como `fact_renta`, `fact_oferta_idealista` ni las m√©tricas ampliadas de edad), y no est√° pensado para ejecutarse autom√°ticamente en CI o como parte del pipeline ETL.

### üìù Pasos para Implementar

1. Refactorizar `verify_integrity.py` para usar `logging` en lugar de `print()`, siguiendo el formato y niveles del resto del proyecto.  
2. A√±adir checks estructurados para:
   - Duplicados en `fact_precios` usando la clave √∫nica real (incluyendo `dataset_id` y `source`).  
   - Nulls cr√≠ticos en `fact_demografia` (edad_media, densidad_hab_km2, hogares_totales, nuevas m√©tricas de edad).  
   - Rango razonable de valores (p.ej. precios > 0, densidades positivas, porcentajes entre 0 y 100).  
   - Integridad referencial entre `fact_*` y `dim_barrios`.  
3. Incorporar m√©tricas de resumen (conteos por tabla, % de nulls por columna cr√≠tica) en un peque√±o reporte legible.  
4. Exponer una interfaz CLI clara (por ejemplo `python scripts/verify_integrity.py --db data/processed/database.db`) con c√≥digos de salida apropiados (0=OK, 1=warnings/errores).  
5. Documentar el uso del script en `docs/PROJECT_STATUS.md` o en un documento espec√≠fico de calidad de datos.  
6. (Opcional) A√±adir una job de CI futura que ejecute este script tras el ETL.

### ‚úÖ Definici√≥n de Hecho (Definition of Done)

- [ ] `verify_integrity.py` usa `logging` y no `print()`.  
- [ ] Existen checks para todas las tablas principales y campos cr√≠ticos definidos en el sprint de integridad.  
- [ ] El script devuelve un c√≥digo de salida no cero cuando hay problemas graves de integridad.  
- [ ] Hay ejemplos de salida/documentaci√≥n para interpretar los resultados.  
- [ ] La ejecuci√≥n local del script sobre la base de datos actual produce un resumen claro y accionable.

### üéØ Impacto & KPI

- **KPI afectado:** % de ejecuciones ETL que pasan los checks de integridad sin errores graves.  
- **Impacto directo:** Reduce riesgo de degradaci√≥n silenciosa de datos y facilita debugging de problemas en pipelines futuros.  
- **Facilita:** Integraci√≥n con CI/CD y validaciones previas a releases.

### üîó Issues Relacionadas

- Relacionada con: sprint de integridad de datos (deduplicaci√≥n `fact_precios`, enriquecimiento `fact_demografia`).  
- Puede alimentar futuras issues de testing y calidad de datos.

### üöß Riesgos / Bloqueos

- Posible necesidad de ajustar umbrales de aceptaci√≥n (p.ej. % m√°ximo de nulls) a medida que se incorporan nuevas fuentes.  
- Riesgo de falsos positivos si los checks no tienen en cuenta casos especiales documentados.

### üìö Enlaces Relevantes

- `scripts/verify_integrity.py`  
- `src/data_processing.py` y `src/transform/cleaners.py`  
- `docs/PROJECT_STATUS.md` (secci√≥n Issues Identificados y Pr√≥ximos Pasos)  

### ‚è±Ô∏è Tiempo Estimado

**3-5 horas**
'


#
# Issue 3: Tests de integraci√≥n para pipeline ETL (fact_precios y fact_demografia)
#
gh issue create \
  --title "üìã [S1] Tests de integraci√≥n para pipeline ETL (fact_precios y fact_demografia)" \
  --label "sprint-1" \
  --label "testing" \
  --label "etl" \
  --label "data-processing" \
  --label "priority-high" \
  --assignee "${current_user}" \
  --body '
### üìå Objetivo

Cubrir con tests de integraci√≥n el pipeline ETL que construye `fact_precios` y `fact_demografia`, incluyendo las funciones de enriquecimiento (`enrich_fact_demografia`) y la integraci√≥n de Portal de Dades, para garantizar que las regresiones en deduplicaci√≥n y enriquecimiento se detectan autom√°ticamente.

### üîç Descripci√≥n del Problema

El m√≥dulo `src/data_processing.py` y los helpers en `src/transform/cleaners.py` ya implementan l√≥gica avanzada de deduplicaci√≥n, normalizaci√≥n de barrios y enriquecimiento de campos demogr√°ficos usando Portal de Dades. Sin embargo, la cobertura de tests de integraci√≥n sobre el pipeline completo sigue siendo limitada, lo que deja espacio a regresiones silenciosas (por ejemplo, cambios en estructura de CSV, nuevos indicadores, variaciones de nombres de territorios o ajustes en deduplicaci√≥n sem√°ntica).

### üìù Pasos para Implementar

1. Dise√±ar datasets m√≠nimos de prueba (fixtures en `tests/fixtures/`) que representen:
   - M√∫ltiples fuentes de precios (Open Data BCN + Portal de Dades) con potencial solapamiento.  
   - Casos con barrios dif√≠ciles de mapear y alias.  
   - Series demogr√°ficas con huecos que deban rellenarse v√≠a `enrich_fact_demografia`.  
2. Crear tests de integraci√≥n para:
   - `prepare_fact_precios()` verificando que:
     - Se preserva la granularidad multi-fuente (no se pierden registros v√°lidos).  
     - La deduplicaci√≥n solo elimina duplicados exactos (`dataset_id`, `source`, `trimestre`).  
   - `prepare_fact_demografia()` y `enrich_fact_demografia()` verificando:
     - Relleno de `hogares_totales`, `edad_media`, `porc_inmigracion`, `densidad_hab_km2`.  
     - Nuevas m√©tricas de edad (`pct_mayores_65`, etc.) cuando hay datos raw disponibles.  
3. A√±adir asserts sobre:
   - N√∫mero esperado de filas en tablas de hechos.  
   - % m√°ximo de nulls permitido en campos clave (<10% seg√∫n criterios de integridad).  
   - Rango razonable de valores (sin negativos, porcentajes 0-100).  
4. Integrar estos tests en `tests/test_pipeline.py` o crear un nuevo m√≥dulo dedicado.  
5. Documentar los supuestos de los fixtures y la intenci√≥n de los tests en un breve README dentro de `tests/fixtures/`.

### ‚úÖ Definici√≥n de Hecho (Definition of Done)

- [ ] Existen tests que ejecutan de extremo a extremo las funciones clave del pipeline ETL para precios y demograf√≠a.  
- [ ] Los tests fallan de forma clara si se rompe la deduplicaci√≥n sem√°ntica o el enriquecimiento de campos.  
- [ ] La ejecuci√≥n de `pytest` incluye estos tests sin aumentar excesivamente el tiempo total.  
- [ ] Los fixtures de datos de prueba est√°n documentados y versionados junto con el c√≥digo.

### üéØ Impacto & KPI

- **KPI afectado:** Cobertura de tests del pipeline ETL y % de regresiones detectadas antes de producci√≥n.  
- **Impacto directo:** Mayor confianza en cambios futuros de deduplicaci√≥n, mapeo de territorios y enriquecimiento demogr√°fico.  
- **Facilita:** Refactors seguros y experimentaci√≥n con nuevas fuentes de datos.

### üîó Issues Relacionadas

- Relacionada con: sprint de integridad de datos (`fact_precios`, `fact_demografia`).  
- Conecta con futuras issues de CI/CD y m√©tricas de calidad.

### üöß Riesgos / Bloqueos

- Dise√±o de fixtures demasiado complejos que hagan dif√≠ciles de mantener los tests.  
- Posible necesidad de refactorizar funciones para hacerlas m√°s f√°cilmente testeables.

### üìö Enlaces Relevantes

- `src/data_processing.py`  
- `src/transform/cleaners.py`  
- `tests/test_pipeline.py`, `tests/test_cleaners.py`  
- `docs/PROJECT_STATUS.md` (secciones de integridad y pr√≥ximos pasos)  

### ‚è±Ô∏è Tiempo Estimado

**4-6 horas**
'


#
# Issue 4: Implementar extractor completo para INE
#
gh issue create \
  --title "üìã [S1] Implementar extractor completo para INE (series hist√≥ricas de referencia)" \
  --label "sprint-1" \
  --label "data-extraction" \
  --label "priority-medium" \
  --assignee "${current_user}" \
  --body '
### üìå Objetivo

Implementar un extractor completo para el INE que obtenga series hist√≥ricas de precios y/o indicadores demogr√°ficos a nivel municipal/nacional, para usarlos como benchmark y contexto frente a los datos de barrios/distritos de Barcelona.

### üîç Descripci√≥n del Problema

Seg√∫n `docs/PROJECT_STATUS.md`, el `INEExtractor` sigue en versi√≥n base y no se han automatizado las descargas de precios hist√≥ricos nacionales. Actualmente el proyecto depende principalmente del Portal de Dades y Open Data BCN para series largas, lo que limita la capacidad de comparar la evoluci√≥n de Barcelona frente a tendencias m√°s amplias (Catalu√±a, Espa√±a). Contar con un extractor de INE robusto permitir√≠a enriquecer an√°lisis de contexto y validar la coherencia de las series locales.

### üìù Pasos para Implementar

1. Revisar el dise√±o actual de extractores en `src/data_extraction.py` y `src/extraction/` para alinear el `INEExtractor` con el patr√≥n `BaseExtractor`.  
2. Investigar los endpoints relevantes del INE (precios de vivienda, renta, demograf√≠a) y documentar los IDs de serie necesarios.  
3. Implementar el `INEExtractor` con:
   - Manejo de paginaci√≥n/rate limits.  
   - Guardado de respuestas raw en `data/raw/ine/` con timestamp.  
   - Logs claros de cobertura temporal y tama√±o de datos.  
4. Crear funciones de procesamiento inicial (pueden ser simples) para transformar los datos raw en un formato compatible con el esquema actual o en tablas auxiliares.  
5. A√±adir tests unitarios y, si es posible, un peque√±o test de integraci√≥n con datos mockeados para evitar depender de la API real en CI.  
6. Documentar el extractor en `docs/sources/ine.md` (similar a IDESCAT) y enlazarlo desde la documentaci√≥n general de fuentes.

### ‚úÖ Definici√≥n de Hecho (Definition of Done)

- [ ] `INEExtractor` implementado siguiendo el patr√≥n `BaseExtractor` y probado localmente.  
- [ ] Datos raw guardados en `data/raw/ine/` con estructura consistente.  
- [ ] Al menos una serie hist√≥rica relevante (precios o renta) descargada y disponible para an√°lisis.  
- [ ] Tests unitarios b√°sicos pasando (incluyendo manejo de errores y de respuestas vac√≠as).  
- [ ] Documentaci√≥n de la fuente y de los endpoints utilizada.

### üéØ Impacto & KPI

- **KPI afectado:** N√∫mero de fuentes externas integradas (objetivo: 3/4 a corto plazo).  
- **Impacto directo:** Mejora la capacidad de contextualizar los datos de Barcelona compar√°ndolos con tendencias nacionales/municipales.  
- **Facilita:** An√°lisis posteriores de convergencia/divergencia de precios y renta.

### üîó Issues Relacionadas

- Relacionada con: an√°lisis de correlaciones y EDA avanzada.  
- Conecta con futuras issues de an√°lisis comparativo (`src/analysis.py`).

### üöß Riesgos / Bloqueos

- Complejidad de la API del INE (documentaci√≥n, autenticaci√≥n, formatos de respuesta heterog√©neos).  
- Cambios de estructura/IDs de serie con el tiempo.  
- Posibles l√≠mites de peticiones o ventanas de mantenimiento del servicio.

### üìö Enlaces Relevantes

- `src/data_extraction.py`, `src/extraction/`  
- `docs/PROJECT_STATUS.md` (secci√≥n Datos de INE pendientes)  
- Documentaci√≥n oficial del INE (API)  

### ‚è±Ô∏è Tiempo Estimado

**1-2 d√≠as**
'


